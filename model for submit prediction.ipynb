{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMkw9BQ6I0MTkWO95ZqX+57"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dJ7SahnlJfJ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724306231526,"user_tz":-540,"elapsed":25706,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"d1350e5f-c97f-4233-f32d-7d9c461e275b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["> ### 라이브러리 호출"],"metadata":{"id":"K7_t2x7MYlkF"}},{"cell_type":"code","source":["import os\n","from pprint import pprint\n","\n","# 데이터 전처리 및 모델링\n","import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","# 시각화\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#불필요한 경고 제거\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"O6mTbI9mYqrd","executionInfo":{"status":"ok","timestamp":1724306238811,"user_tz":-540,"elapsed":7292,"user":{"displayName":"이찬","userId":"00141605834555734115"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def read_excel_file(file_path: str, header: int = None) -> pd.DataFrame:\n","    csv_file = file_path.replace(\".xlsx\", \".csv\")\n","\n","    if not os.path.exists(csv_file):\n","        print(\"Converting excel to csv...\")\n","        if header:\n","            df = pd.read_excel(file_path, header=header)\n","        else:\n","            df = pd.read_excel(file_path)\n","\n","        df.to_csv(csv_file, index=False)\n","        print(f\"  {file_path} -> {csv_file}\")\n","        return df\n","    else:\n","        print(f\"  Reading {csv_file}\")\n","        return pd.read_csv(csv_file, low_memory=False)"],"metadata":{"id":"AcN2b0KoY1xn","executionInfo":{"status":"ok","timestamp":1724306238811,"user_tz":-540,"elapsed":12,"user":{"displayName":"이찬","userId":"00141605834555734115"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Columns 리스트 생략 없이 전부 출력하기\n","pd.set_option('display.max_seq_items', None)\n","# col 생략 없이 출력\n","pd.set_option('display.max_columns', None)"],"metadata":{"id":"t2_qji-9Y35d","executionInfo":{"status":"ok","timestamp":1724306238811,"user_tz":-540,"elapsed":5,"user":{"displayName":"이찬","userId":"00141605834555734115"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["ROOT_DIR = \"/content/drive/MyDrive/LG AImers/data set/\"\n","RANDOM_STATE = 110\n","\n","# Load_train_data\n","train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))"],"metadata":{"id":"njWuR5vOY5ao","executionInfo":{"status":"ok","timestamp":1724306245382,"user_tz":-540,"elapsed":6576,"user":{"displayName":"이찬","userId":"00141605834555734115"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Load_test_data\n","test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n","submission_df = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))"],"metadata":{"id":"rcbeZZNGEUC9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터 전처리"],"metadata":{"id":"F6ru0egEZ2wl"}},{"cell_type":"markdown","source":["### STEP 1. Column의 값이 모두 Nan이거나 값이 모두 같은 feature 제거"],"metadata":{"id":"Vqp2hVsokuhJ"}},{"cell_type":"code","source":["def remove_constant_columns(df: pd.DataFrame) -> pd.DataFrame:\n","  # 값이 모두 NaN인 열 제거\n","  df = df.dropna(axis=1, how='all')\n","\n","  # NaN을 포함하여 unique한 값이 2개 이상인 열만 선택\n","  df = df.loc[:, df.apply(lambda x: x.nunique(dropna=False)) > 1]\n","\n","  return df"],"metadata":{"id":"k-vD9zJ5dwAc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 2. Solve Column Shift Problem\n","\n","- HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result 값이 OK / NaN인 값은 모두 HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result 칼럼부터 WorkMode Collect Result까지 data 값이 하나씩 밀려있다.\n","\n","> 1. split_dataframe_by_column_suffix => dam / fill1 / fill2 / autoclave 분리\n","2. shift_columns_forward_conditionally_dam => dam 관련 data 오류 해결\n","3. shift_columns_forward_conditionally_fill1  => fill1 관련 data 오류 해결\n","4. shift_columns_forward_conditionally_fill2 => fill2 관련 data 오류 해결\n","\n","> **결측값 처리**\n","- 전체 40506개의 data 중 24059개의 결측값이 존재하기에 drop\n","- HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value & GMES_ORIGIN_INSP_JUDGE_CODE 관련 컬럼 DROP\n","- **WorkMode Collect Result_Dam, WorkMode Collect Result_Fill1, WorkMode Collect Result_Fill2 칼럼 DROP**"],"metadata":{"id":"UEwsITcBlHez"}},{"cell_type":"code","source":["# HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value 관련 컬럼 DROP\n","# GMES_ORIGIN_INSP_JUDGE_CODE 관련 컬럼 DROP\n","\n","def drop_columns_with_string(df, substring):\n","  df_filtered = df.drop(columns=[col for col in df.columns if substring in col])\n","  return df_filtered"],"metadata":{"id":"X5j2fKKJBAq1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> 기존 dataframe에서 target column 추출"],"metadata":{"id":"Ok1ylE6zENK0"}},{"cell_type":"code","source":["train_data_target_df = train_data[\"target\"]"],"metadata":{"id":"ektMql3-GCVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_dataframe_by_column_suffix(df):\n","    \"\"\"\n","    주어진 데이터프레임에서 컬럼명이 'Dam', 'Fill1', 'Fill2', 'AutoClave'로 끝나는 열들로\n","    구성된 4개의 데이터프레임으로 나누어 반환합니다.\n","\n","    Parameters:\n","    - df: pd.DataFrame - 원본 데이터프레임\n","\n","    Returns:\n","    - pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame - 나눠진 4개의 데이터프레임\n","    \"\"\"\n","    # 'Dam'으로 끝나는 칼럼\n","    dam_columns = [col for col in df.columns if col.endswith('Dam')]\n","    df_dam = df[dam_columns]\n","\n","    # 'Fill1'로 끝나는 칼럼\n","    fill1_columns = [col for col in df.columns if col.endswith('Fill1')]\n","    df_fill1 = df[fill1_columns]\n","\n","    # 'Fill2'로 끝나는 칼럼\n","    fill2_columns = [col for col in df.columns if col.endswith('Fill2')]\n","    df_fill2 = df[fill2_columns]\n","\n","    # 'AutoClave'로 끝나는 칼럼\n","    autoclave_columns = [col for col in df.columns if col.endswith('AutoClave')]\n","    df_autoclave = df[autoclave_columns]\n","\n","    return df_dam, df_fill1, df_fill2, df_autoclave"],"metadata":{"id":"mG0iQtucltzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","def shift_columns_forward_conditionally_dam(df):\n","    # 복사본을 생성하여 원본을 유지합니다.\n","    df_shifted = df.copy()\n","\n","    # 조건에 해당하는 행들을 필터링\n","    condition = df[\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\"].isin([\"OK\"]) | df[\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\"].isna()\n","\n","    # 컬럼 이름 목록\n","    columns_to_shift = [\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam\",\n","        \"HEAD Standby Position X Collect Result_Dam\",\n","        \"HEAD Standby Position Y Collect Result_Dam\",\n","        \"HEAD Standby Position Z Collect Result_Dam\",\n","        \"Head Clean Position X Collect Result_Dam\",\n","        \"Head Clean Position Y Collect Result_Dam\",\n","        \"Head Clean Position Z Collect Result_Dam\",\n","        \"Head Purge Position X Collect Result_Dam\",\n","        \"Head Purge Position Y Collect Result_Dam\",\n","        \"Head Purge Position Z Collect Result_Dam\",\n","        \"Head Zero Position X Collect Result_Dam\",\n","        \"Head Zero Position Y Collect Result_Dam\",\n","        \"Head Zero Position Z Collect Result_Dam\",\n","        \"Machine Tact time Collect Result_Dam\",\n","        \"PalletID Collect Result_Dam\",\n","        \"Production Qty Collect Result_Dam\",\n","        \"Receip No Collect Result_Dam\",\n","        \"Stage1 Circle1 Distance Speed Collect Result_Dam\",\n","        \"Stage1 Circle2 Distance Speed Collect Result_Dam\",\n","        \"Stage1 Circle3 Distance Speed Collect Result_Dam\",\n","        \"Stage1 Circle4 Distance Speed Collect Result_Dam\",\n","        \"Stage1 Line1 Distance Speed Collect Result_Dam\",\n","        \"Stage1 Line2 Distance Speed Collect Result_Dam\",\n","        \"Stage1 Line3 Distance Speed Collect Result_Dam\",\n","        \"Stage1 Line4 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Circle1 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Circle2 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Circle3 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Circle4 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Line1 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Line2 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Line3 Distance Speed Collect Result_Dam\",\n","        \"Stage2 Line4 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Circle1 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Circle2 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Circle3 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Circle4 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Line1 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Line2 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Line3 Distance Speed Collect Result_Dam\",\n","        \"Stage3 Line4 Distance Speed Collect Result_Dam\",\n","        \"THICKNESS 1 Collect Result_Dam\",\n","        \"THICKNESS 2 Collect Result_Dam\",\n","        \"THICKNESS 3 Collect Result_Dam\",\n","        \"WorkMode Collect Result_Dam\"\n","    ]\n","\n","\n","    # 조건을 만족하는 행들에 대해서만 값을 이동\n","    for i in range(len(columns_to_shift) - 1):\n","        df_shifted.loc[condition, columns_to_shift[i]] = df_shifted.loc[condition, columns_to_shift[i + 1]]\n","\n","    # 마지막 컬럼을 NaN으로 설정\n","    df_shifted.loc[condition, columns_to_shift[-1]] = pd.NA\n","\n","    return df_shifted\n"],"metadata":{"id":"XwI_NWFSl_st"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def shift_columns_forward_conditionally_fill1(df):\n","    # 복사본을 생성하여 원본을 유지합니다.\n","    df_shifted = df.copy()\n","\n","    # 조건에 해당하는 행들을 필터링\n","    condition = df[\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\"].isin([\"OK\"]) | df[\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\"].isna()\n","\n","    # 컬럼 이름 목록\n","    columns_to_shift = [\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1\",\n","        \"HEAD Standby Position X Collect Result_Fill1\",\n","        \"HEAD Standby Position Y Collect Result_Fill1\",\n","        \"HEAD Standby Position Z Collect Result_Fill1\",\n","        \"Head Clean Position X Collect Result_Fill1\",\n","        \"Head Clean Position Y Collect Result_Fill1\",\n","        \"Head Clean Position Z Collect Result_Fill1\",\n","        \"Head Purge Position X Collect Result_Fill1\",\n","        \"Head Purge Position Y Collect Result_Fill1\",\n","        \"Head Purge Position Z Collect Result_Fill1\",\n","        \"Machine Tact time Collect Result_Fill1\",\n","        \"PalletID Collect Result_Fill1\",\n","        \"Production Qty Collect Result_Fill1\",\n","        \"Receip No Collect Result_Fill1\",\n","        \"WorkMode Collect Result_Fill1\"\n","    ]\n","\n","    # 조건을 만족하는 행들에 대해서만 값을 이동\n","    for i in range(len(columns_to_shift) - 1):\n","        df_shifted.loc[condition, columns_to_shift[i]] = df_shifted.loc[condition, columns_to_shift[i + 1]]\n","\n","    # 마지막 컬럼을 pd.NA로 설정\n","    df_shifted.loc[condition, columns_to_shift[-1]] = pd.NA\n","\n","    return df_shifted\n"],"metadata":{"id":"JyJRd0HImqny"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def shift_columns_forward_conditionally_fill2(df):\n","    # 복사본을 생성하여 원본을 유지합니다.\n","    df_shifted = df.copy()\n","\n","    # 조건에 해당하는 행들을 필터링\n","    condition = df[\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\"].isin([\"OK\"]) | df[\"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\"].isna()\n","\n","    # 새로운 컬럼 이름 목록\n","    columns_to_shift = [\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2\",\n","        \"HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2\",\n","        \"HEAD Standby Position X Collect Result_Fill2\",\n","        \"HEAD Standby Position Y Collect Result_Fill2\",\n","        \"HEAD Standby Position Z Collect Result_Fill2\",\n","        \"Head Clean Position X Collect Result_Fill2\",\n","        \"Head Clean Position Y Collect Result_Fill2\",\n","        \"Head Clean Position Z Collect Result_Fill2\",\n","        \"Head Purge Position X Collect Result_Fill2\",\n","        \"Head Purge Position Y Collect Result_Fill2\",\n","        \"Head Purge Position Z Collect Result_Fill2\",\n","        \"Machine Tact time Collect Result_Fill2\",\n","        \"PalletID Collect Result_Fill2\",\n","        \"Production Qty Collect Result_Fill2\",\n","        \"Receip No Collect Result_Fill2\",\n","        \"WorkMode Collect Result_Fill2\"\n","    ]\n","\n","    # 조건을 만족하는 행들에 대해서만 값을 이동\n","    for i in range(len(columns_to_shift) - 1):\n","        df_shifted.loc[condition, columns_to_shift[i]] = df_shifted.loc[condition, columns_to_shift[i + 1]]\n","\n","    # 마지막 컬럼을 pd.NA로 설정\n","    df_shifted.loc[condition, columns_to_shift[-1]] = pd.NA\n","\n","    return df_shifted\n"],"metadata":{"id":"pZJt-P6Wmvst"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 3. Data Frame 병합 후 WorkMode 관련 Column Drop"],"metadata":{"id":"QMEATPFQnTyF"}},{"cell_type":"code","source":["def concatenate_dataframes(*dfs):\n","  # 데이터프레임의 개수가 4개 또는 5개인지 확인\n","  if len(dfs) not in [4, 5]:\n","    raise ValueError(\"입력 데이터프레임의 개수는 target을 포함한 5개이거나 target을 제외한 4개여야 합니다.\")\n","\n","  # 모든 데이터프레임이 동일한 행의 개수를 가지는지 확인\n","  row_counts = [df.shape[0] for df in dfs]\n","  if len(set(row_counts)) != 1:\n","    raise ValueError(\"모든 데이터프레임의 행의 개수가 동일해야 합니다.\")\n","\n","  # 데이터프레임들을 옆으로 붙이기\n","  merged_data_frame = pd.concat(dfs, axis=1)\n","\n","  # 'WorkMode'가 컬럼명에 포함된 모든 컬럼을 드롭(drop)\n","  columns_to_drop = [col for col in merged_data_frame.columns if 'WorkMode' in col]\n","  merged_data_frame_drop_workmode_col = merged_data_frame.drop(columns = columns_to_drop)\n","\n","  return merged_data_frame_drop_workmode_col"],"metadata":{"id":"-a-2bJrRnb01"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result' 관련 컬럼의 데이터 타입 숫자형으로 변경**\n","- Column Shift 문제를 해결한 후 HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result 관련 컬럼들의 data type이 모두 category형으로 남아있기에 float 타입으로 변경해준다."],"metadata":{"id":"XPEbKOaFlyZ4"}},{"cell_type":"code","source":["def convert_columns_to_float(df, keyword):\n","    \"\"\"\n","    Parameters:\n","    - df (pd.DataFrame): 변환할 데이터 프레임\n","    - keyword (str): 칼럼명에 포함된 키워드\n","\n","    Returns:\n","    - pd.DataFrame: 변환된 데이터 프레임\n","    \"\"\"\n","    # 키워드를 포함하는 칼럼명을 찾기\n","    columns_to_convert = [col for col in df.columns if keyword in col]\n","\n","    # 해당 칼럼들을 실수형으로 변환\n","    df[columns_to_convert] = df[columns_to_convert].astype(float)\n","\n","    return df"],"metadata":{"id":"rzIzZa-Fl1Gj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 4"],"metadata":{"id":"1NZ-U71Pa09N"}},{"cell_type":"markdown","source":["#### STEP 4-1. 제조 공정 중 Qty 변화 있으면 AbNormal"],"metadata":{"id":"7Ctzgl31P4Sl"}},{"cell_type":"code","source":["def filter_different_rows_Qty(df):\n","    # 제조 공정 중 Qty 칼럼값에 변화가 있는 row\n","    condition = (df['Production Qty Collect Result_Dam'] != df['Production Qty Collect Result_Fill1']) | \\\n","                (df['Production Qty Collect Result_Dam'] != df['Production Qty Collect Result_Fill2']) | \\\n","                (df['Production Qty Collect Result_Fill1'] != df['Production Qty Collect Result_Fill2'])\n","\n","    # 조건을 만족하는 행들로 구성된 데이터프레임\n","    filtered_df = df[condition]\n","\n","    # 입력 데이터프레임과 필터링된 데이터프레임 반환\n","    return filtered_df, df"],"metadata":{"id":"tQpyKkx8QC0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### STEP 4-2. 제조 공정 중 Pallet ID 변화 있으면 AbNormal"],"metadata":{"id":"eXotNG1daipn"}},{"cell_type":"code","source":["def filter_different_rows_PalletID(df):\n","    # 제조 공정 중 Pallet ID 칼럼값에 변화가 있는 row\n","    condition = (df['PalletID Collect Result_Dam'] != df['PalletID Collect Result_Fill1']) | \\\n","                (df['PalletID Collect Result_Dam'] != df['PalletID Collect Result_Fill2']) | \\\n","                (df['PalletID Collect Result_Fill1'] != df['PalletID Collect Result_Fill2'])\n","\n","    # 조건을 만족하는 행들로 구성된 데이터프레임\n","    filtered_df = df[condition]\n","\n","    # 입력 데이터프레임과 필터링된 데이터프레임 반환\n","    return filtered_df, df"],"metadata":{"id":"E4fQ-FZIahLE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### STEP 4-3. 제조 공정 중 Receip No 변화 있으면 AbNormal"],"metadata":{"id":"uxcSYwJma7E6"}},{"cell_type":"code","source":["def filter_different_rows_Receip(df):\n","    # 제조 공정 중 Receip No 칼럼값에 변화가 있는 row\n","    condition = (df['Receip No Collect Result_Dam'] != df['Receip No Collect Result_Fill1']) | \\\n","                (df['Receip No Collect Result_Dam'] != df['Receip No Collect Result_Fill2']) | \\\n","                (df['Receip No Collect Result_Fill1'] != df['Receip No Collect Result_Fill2'])\n","\n","    # 조건을 만족하는 행들로 구성된 데이터프레임\n","    filtered_df = df[condition]\n","\n","    # 입력 데이터프레임과 필터링된 데이터프레임 반환\n","    return filtered_df, df"],"metadata":{"id":"HXRyBI9Za83D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### STEP 4-4. 제조 공정 중 Equipment 변화 있으면 AbNormal"],"metadata":{"id":"ogUFjCjGWOXP"}},{"cell_type":"code","source":["def filter_different_rows_Equipment(df):\n","    # 각 칼럼의 값의 마지막 글자 추출\n","    last_char_dam = df['Equipment_Dam'].astype(str).str[-1]\n","    last_char_fill1 = df['Equipment_Fill1'].astype(str).str[-1]\n","    last_char_fill2 = df['Equipment_Fill2'].astype(str).str[-1]\n","\n","    # 제조 공정 중 Equipment 칼럼값에 변화가 있는 row\n","    condition = (last_char_dam != last_char_fill1) | (last_char_dam != last_char_fill2) | (last_char_fill1 != last_char_fill2)\n","\n","    # 조건을 만족하는 행들로 구성된 데이터프레임\n","    filtered_df = df[condition]\n","\n","    # 입력 데이터프레임과 필터링된 데이터프레임 반환\n","    return filtered_df, df"],"metadata":{"id":"RcgGZ_v5WVqQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 5. 무조건 AbNormal인 값들의 index 저장하기"],"metadata":{"id":"46iVDvNyQPMD"}},{"cell_type":"markdown","source":["> 무조건 AbNormal인 row의 index들을 저장한 후, 학습 과정에서는 제외시킨 후 이후에 최종 과정에서 바꿔준다."],"metadata":{"id":"KZnDM_usN76c"}},{"cell_type":"code","source":["# 무조건 Error가 발생하는 데이터들의 row index를 반환하는 함수\n","def get_unique_row_indices_with_AbNormal(*dfs):\n","    \"\"\"\n","    dfs (tuple): 데이터 프레임들의 튜플\n","    ex) AbNormal_row_index = get_unique_row_indices(data_with_dispensesr_error_df, data_with_Qty_error_df)\n","    \"\"\"\n","    # 중복을 제거하기 위해 set 사용\n","    unique_indices = set()\n","\n","    # 각 데이터 프레임에 대해 인덱스를 추출하여 set에 추가\n","    for df in dfs:\n","      unique_indices.update(df.index)\n","\n","    # 중복 없는 인덱스를 리스트로 변환하여 반환\n","    return list(unique_indices)"],"metadata":{"id":"Eb16kLCjOYrN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Change in the Column 파생변수 생성**"],"metadata":{"id":"gzUTbA7ffhC8"}},{"cell_type":"code","source":["def add_change_column(df, AbNormal_row_index_list):\n","  df['Change in the Column'] = df.index.isin(AbNormal_row_index_list).astype(int)\n","  return df"],"metadata":{"id":"8bs_NAZifSFr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 6. Dispenser의 종류에 따라 Data Frame Split"],"metadata":{"id":"1DKqCCP8Ks1i"}},{"cell_type":"code","source":["# Equipment_Dam를 각 제품들의 original 설정값이라는 가정하에, dam에서의 dispenser에 따라 데이터프레임 분할\n","def Seperate_data_by_Dam_dispenser(df):\n","  # Equipment_Dam 칼럼의 값이 \"Dam dispenser #1\"인 경우\n","  df_dispenser_1 = df[df['Equipment_Dam'] == 'Dam dispenser #1']\n","\n","  # Equipment_Dam 칼럼의 값이 \"Dam dispenser #2\"인 경우\n","  df_dispenser_2 = df[df['Equipment_Dam'] == 'Dam dispenser #2']\n","\n","  # 각각의 분할된 데이터프레임 반환\n","  return df_dispenser_1, df_dispenser_2"],"metadata":{"id":"W2G59ADkOP92"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Dispenser에 따라 나눈 후, Equipment 관련 컬럼 DROP**\n","\n","```python\n","dispenser1_df_train = drop_columns_with_string(dispenser1_df_train, \"Equipment\")\n","dispenser2_df_train = drop_columns_with_string(dispenser2_df_train, \"Equipment\")\n","```\n","\n"],"metadata":{"id":"VjQ5JZLH3X6c"}},{"cell_type":"markdown","source":["### STEP 7. Dispenser 구별 후 행이 같은 경우에 중복되는 값을 가지는 Columns DROP"],"metadata":{"id":"dQp_bxvb0o6j"}},{"cell_type":"code","source":["def find_identical_value_columns(df):\n","  # 결과를 저장할 딕셔너리 생성\n","  identical_columns = {}\n","\n","  # 데이터 프레임의 모든 칼럼을 순회\n","  for i, col1 in enumerate(df.columns):\n","    for col2 in df.columns[i+1:]:\n","      # 두 칼럼의 값이 동일한지 확인\n","      if df[col1].equals(df[col2]):\n","        # 동일한 값을 가지는 칼럼을 리스트에 추가\n","        if col1 not in identical_columns:\n","          identical_columns[col1] = []\n","          identical_columns[col1].append(col2)\n","\n","  return identical_columns"],"metadata":{"id":"lbDCUq0p0oUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import defaultdict\n","\n","def build_graph(identical_columns):\n","    graph = defaultdict(set)\n","\n","    for key, values in identical_columns.items():\n","        for value in values:\n","            graph[key].add(value)\n","            graph[value].add(key)\n","\n","    return graph\n","\n","def find_connected_components(graph):\n","    visited = set()\n","    components = []\n","\n","    def dfs(node, component):\n","        stack = [node]\n","        while stack:\n","            v = stack.pop()\n","            if v not in visited:\n","                visited.add(v)\n","                component.append(v)\n","                stack.extend(graph[v] - visited)\n","\n","    for node in graph:\n","        if node not in visited:\n","            component = []\n","            dfs(node, component)\n","            components.append(component)\n","\n","    return components"],"metadata":{"id":"FG3MXlL_1uVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def drop_duplicate_columns(df, components):\n","  # Collect columns to drop\n","  columns_to_drop = set()\n","  for component in components:\n","    if len(component) > 1:\n","      # Skip the first column, mark others for dropping\n","      columns_to_drop.update(component[1:])\n","\n","  # Drop columns from the DataFrame\n","  final_df = df.drop(columns=columns_to_drop, errors='ignore')\n","\n","  return final_df"],"metadata":{"id":"KttWtpl-2HAF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 8. Dispenser 구별 후 UNIQUE한 값을 1개만 가지는 Columns DROP"],"metadata":{"id":"VAR0eH-c0Zhx"}},{"cell_type":"code","source":["# 주어진 데이터프레임에서 고유한 값이 1개인 칼럼을 제거하고 새로운 데이터프레임을 반환\n","def drop_single_unique_columns(df):\n","  # 각 칼럼의 고유 값 개수를 확인\n","  unique_counts = df.nunique()\n","\n","  # 고유 값이 1개인 칼럼 이름 찾기\n","  single_unique_cols = unique_counts[unique_counts == 1].index\n","\n","  # 해당 칼럼을 제거한 새로운 데이터프레임 생성\n","  df_dropped = df.drop(columns=single_unique_cols)\n","  return df_dropped"],"metadata":{"id":"htUPaack0u1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### STEP 9. Target Column 및 Object 데이터 타입 Column들 데이터 타입 변경\n","- Normal => 0\n","- AbNormal => 1\n","- 칼럼의 데이터 타입을 정수형으로 변환"],"metadata":{"id":"wkxqYEuZQ7iK"}},{"cell_type":"code","source":["# Object data type => category형으로 변환\n","def convert_to_category(df):\n","  \"\"\"\n","  주어진 데이터 프레임에서 target 칼럼을 제외한 수치형이 아닌 모든 칼럼에 대해 범주형으로 변환\n","  Returns : pandas.DataFrame: 범주형으로 변환된 데이터 프레임\n","  \"\"\"\n","  # 'target' 칼럼이 있는 경우, 이를 제외한 나머지 칼럼들을 처리\n","  if 'target' in df.columns:\n","    columns_to_convert = df.select_dtypes(exclude=['number']).columns.difference(['target'])\n","  else:\n","    columns_to_convert = df.select_dtypes(exclude=['number']).columns\n","\n","  # 수치형이 아닌 칼럼들을 범주형으로 변환\n","  df[columns_to_convert] = df[columns_to_convert].astype('category')\n","\n","  return df"],"metadata":{"id":"ULbMm9q4aa6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# target값 0과 1로 변환\n","def change_target_column_data_type(df):\n","  df[\"target\"] = df[\"target\"].map({\"Normal\": 0, \"AbNormal\": 1})\n","  df[\"target\"] = df[\"target\"].astype(int)\n","  return df"],"metadata":{"id":"BMKUblGERjnI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train Data"],"metadata":{"id":"kb4C3E13AfXB"}},{"cell_type":"code","source":["# Column의 값이 모두 Nan이거나 값이 모두 같은 feature 제거\n","train_data = remove_constant_columns(train_data)\n","\n","# HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value 관련 컬럼 DROP\n","# GMES_ORIGIN_INSP_JUDGE_CODE 관련 컬럼 DROP\n","train_data = drop_columns_with_string(train_data, \"HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value\")\n","train_data = drop_columns_with_string(train_data, \"GMES_ORIGIN_INSP_JUDGE_CODE\")\n","\n","# 기존 dataframe에서 target column 추출\n","train_data_target_df = train_data[\"target\"]\n","\n","#  Solve Column Shift Problem\n","df_dam, df_fill1, df_fill2, df_autoclave = split_dataframe_by_column_suffix(train_data)\n","df_dam_correct = shift_columns_forward_conditionally_dam(df_dam)\n","df_fill1_correct = shift_columns_forward_conditionally_fill1(df_fill1)\n","df_fill2_correct = shift_columns_forward_conditionally_fill2(df_fill2)\n","\n","# dataframe 병합\n","train_data_correct = concatenate_dataframes(df_dam_correct, df_fill1_correct, df_fill2_correct, df_autoclave, train_data_target_df)\n","\n","# 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result' 관련 컬럼의 데이터 타입 숫자형으로 변경\n","train_data_correct = convert_columns_to_float(train_data_correct, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result')\n","\n","#############무조건 AbNormal인 row의 index 저장###############\n","# 제조 공정 중 Qty 변화가 있는 데이터 프레임\n","data_with_Qty_error_df_train, train_data_correct = filter_different_rows_Qty(train_data_correct)\n","\n","# 제조 공정 중 PalletID 변화 있는 데이터 프레임\n","data_with_PalletID_error_df_train, train_data_correct = filter_different_rows_PalletID(train_data_correct)\n","\n","# 제조 공정 중 Receip No 변화 있는 데이터 프레임\n","data_with_Receip_error_df_train, train_data_correct = filter_different_rows_Receip(train_data_correct)\n","\n","# 제조 공정 중 Equipment 변화 있는 데이터 프레임\n","data_with_Equipment_error_df_train, train_data_correct = filter_different_rows_Equipment(train_data_correct)\n","\n","# 무조건 AbNormal인 값들의 index 저장하기\n","AbNormal_row_index_list_train = get_unique_row_indices_with_AbNormal(data_with_Qty_error_df_train, data_with_PalletID_error_df_train, data_with_Receip_error_df_train, data_with_Equipment_error_df_train)\n","\n","# 값에 변화가 있는 row들을 구별해주는 Change in the Column 파생변수 생성\n","train_data_correct = add_change_column(train_data_correct, AbNormal_row_index_list_train)\n","\n","# Dispenser의 종류에 따라 Data Frame Split\n","dispenser1_df_train , dispenser2_df_train = Seperate_data_by_Dam_dispenser(train_data_correct)\n","\n","# Equipment 관련 컬럼 DROP\n","dispenser1_df_train = drop_columns_with_string(dispenser1_df_train, \"Equipment\")\n","dispenser2_df_train = drop_columns_with_string(dispenser2_df_train, \"Equipment\")\n","\n","# 행이 같은 경우에 중복되는 값을 가지는 Columns DROP\n","identical_columns_in_df1_train = find_identical_value_columns(dispenser1_df_train)\n","identical_columns_in_df2_train = find_identical_value_columns(dispenser2_df_train)\n","graph1_train = build_graph(identical_columns_in_df1_train)\n","graph2_train = build_graph(identical_columns_in_df2_train)\n","components1_train = find_connected_components(graph1_train)\n","components2_train = find_connected_components(graph2_train)\n","dispenser1_df_train = drop_duplicate_columns(dispenser1_df_train, components1_train)\n","dispenser2_df_train = drop_duplicate_columns(dispenser2_df_train, components2_train)\n","\n","# UNIQUE한 값을 가지는 Columns DROP\n","dispenser1_df_train = drop_single_unique_columns(dispenser1_df_train)\n","dispenser2_df_train = drop_single_unique_columns(dispenser2_df_train)\n","\n","# Model.Suffix, Workorder 관련 칼럼 DROP\n","dispenser1_df_train = drop_columns_with_string(dispenser1_df_train, \"Model.Suffix\")\n","dispenser1_df_train = drop_columns_with_string(dispenser1_df_train, \"Workorder\")\n","dispenser2_df_train = drop_columns_with_string(dispenser2_df_train, \"Model.Suffix\")\n","dispenser2_df_train = drop_columns_with_string(dispenser2_df_train, \"Workorder\")\n","\n","# Object data type => category형으로 변환\n","dispenser1_df_train = convert_to_category(dispenser1_df_train)\n","dispenser2_df_train = convert_to_category(dispenser2_df_train)\n","\n","# target column -> 0(Normal)과 1(AbNormal)로 변경\n","dispenser1_df_train = change_target_column_data_type(dispenser1_df_train)\n","dispenser2_df_train = change_target_column_data_type(dispenser2_df_train)"],"metadata":{"id":"uA3lIr769xE1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test Data"],"metadata":{"id":"cUs80f5OAmJJ"}},{"cell_type":"code","source":["def filter_columns_for_test_dataset_first(test_data_df, train_data_df):\n","  dataframe_for_Columns = remove_constant_columns(train_data)\n","  dataframe_for_Columns = drop_columns_with_string(dataframe_for_Columns, \"HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value\")\n","  dataframe_for_Columns = drop_columns_with_string(dataframe_for_Columns, \"GMES_ORIGIN_INSP_JUDGE_CODE\")\n","\n","  # train dataset의 컬럼들\n","  first_remain_column_list = dataframe_for_Columns.columns\n","\n","  # 입력 데이터프레임의 칼럼 목록\n","  df_columns = test_data_df.columns\n","\n","  # 데이터프레임의 칼럼 중 train_data에 있는 칼럼만 필터링\n","  columns_to_keep = [col for col in df_columns if col in first_remain_column_list]\n","  filtered_df = test_data_df[columns_to_keep]\n","  return filtered_df"],"metadata":{"id":"A8P-cRZtNoer"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def filter_test_columns(dispenser_df_test, dispenser_df_train):\n","    \"\"\" dispenser_df_test 데이터프레임에서 dispenser1_df_train 데이터프레임에 없는 칼럼을 제거하고 새로운 데이터프레임을 반환\"\"\"\n","    # 훈련 데이터프레임의 칼럼 목록\n","    train_columns = dispenser_df_train.columns\n","    # 테스트 데이터프레임의 칼럼 목록\n","    test_columns = dispenser_df_test.columns\n","\n","    # 훈련 데이터프레임에 존재하는 칼럼만 테스트 데이터프레임에서 선택합니다\n","    columns_to_keep = [col for col in test_columns if col in train_columns]\n","    # 선택한 칼럼만 포함된 새로운 데이터프레임을 생성합니다\n","    filtered_test_df = dispenser_df_test[columns_to_keep]\n","\n","    return filtered_test_df"],"metadata":{"id":"tWXYyqGHNqec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Column shifting problem 해결 전 feauture 통일\n","test_data = filter_columns_for_test_dataset_first(test_data, train_data)\n","\n","#  Solve Column Shift Problem\n","df_dam_test, df_fill1_test, df_fill2_test, df_autoclave_test = split_dataframe_by_column_suffix(test_data)\n","df_dam_correct_test = shift_columns_forward_conditionally_dam(df_dam_test)\n","df_fill1_correct_test = shift_columns_forward_conditionally_fill1(df_fill1_test)\n","df_fill2_correct_test = shift_columns_forward_conditionally_fill2(df_fill2_test)\n","\n","# dataframe 병합\n","test_data_correct = concatenate_dataframes(df_dam_correct_test, df_fill1_correct_test, df_fill2_correct_test, df_autoclave_test)\n","\n","# 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result' 관련 컬럼의 데이터 타입 숫자형으로 변경\n","test_data_correct = convert_columns_to_float(test_data_correct, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result')\n","\n","\n","###########무조건 AbNormal인 row의 index 저장############\n","data_with_Qty_error_df_test, test_data_correct = filter_different_rows_Qty(test_data_correct)\n","\n","# 제조 공정 중 PalletID 변화 있는 데이터 프레임\n","data_with_PalletID_error_df_test, test_data_correct = filter_different_rows_PalletID(test_data_correct)\n","\n","# 제조 공정 중 Receip No 변화 있는 데이터 프레임\n","data_with_Receip_error_df_test, test_data_correct = filter_different_rows_Receip(test_data_correct)\n","\n","# 제조 공정 중 Equipment 변화 있는 데이터 프레임\n","data_with_Equipment_error_df_test, test_data_correct = filter_different_rows_Equipment(test_data_correct)\n","\n","# 무조건 AbNormal인 값들의 index 저장하기\n","AbNormal_row_index_list_test = get_unique_row_indices_with_AbNormal(data_with_Qty_error_df_test, data_with_PalletID_error_df_test, data_with_Receip_error_df_test, data_with_Equipment_error_df_test)\n","\n","# 값에 변화가 있는 row들을 구별해주는 Change in the Column 파생변수 생성\n","test_data_correct = add_change_column(test_data_correct, AbNormal_row_index_list_test)\n","\n","# Dispenser의 종류에 따라 Data Frame Split\n","dispenser1_df_test, dispenser2_df_test = Seperate_data_by_Dam_dispenser(test_data_correct)\n","\n","# train data와 feature 맞춰주기\n","dispenser1_df_test = filter_test_columns(dispenser1_df_test, dispenser1_df_train)\n","dispenser2_df_test = filter_test_columns(dispenser2_df_test, dispenser2_df_train)\n","\n","# Object data type => category형으로 변환\n","dispenser1_df_test = convert_to_category(dispenser1_df_test)\n","dispenser2_df_test = convert_to_category(dispenser2_df_test)"],"metadata":{"id":"j6b3YXoOAt1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fSjO-F-nnWL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EG9f47JbnWOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5yygAzoonWRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y-rGNTLMRVJp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["단순히 Automl"],"metadata":{"id":"QxpUYrrEonWp"}},{"cell_type":"markdown","source":["# 발표 듣고\n","\n","- **discharged time of resin : 0인것. => 무조건 abnormal**\n","\n","- **Thickness에 따라 volume이 중요할거 같은데??**\n","\n","=> **칼럼 분포에서 이 값이 차지하는 비율**\n","\n","==============================\n","- 1. catboost든 1svm이든 예측해서 날리고\n","- 2. auto encoder로 autoclave는 남겨두고 dam, fill1, fill2 차원 축소 시키고\n","- 3. smote로 데이터 늘려서\n","- 예측하면 될듯\n","- 2,3의 순서는 고민을 해봐야 할듯\n","\n","- pressure time이 230 under이면 불량일 확률이 높아보인다\n","- OK : 파랑 , NG : 빨강 , 값 : Collect Result * Unit Time"],"metadata":{"id":"o_6s9fcxwiqj"}},{"cell_type":"markdown","source":["## 해볼 것\n","\n","1. 단순 하이퍼 파라미터 튜닝으로 성능 올리기\n","- 모델 선택 : pycaret\n","- 하이퍼 파라미터 튜닝 : optuna\n","\n","2. 확실히 정상인 것을 제외하고\n","\n","3. SMOTE로 늘리기 => 오토인코더 => 시도\n","\n","4. 오토 인코더 => SMOTE로 늘리기 => 시도\n","\n","https://baechu-story.tistory.com/68"],"metadata":{"id":"vu3ILYIVEnDM"}},{"cell_type":"markdown","source":["gradient boosting or xgboost(scale_pos_weight 매개변수, 고차원 data, 수치형 data)를 써보는 게 좋을 것 같다. category형도 없으니까!\n","\n","+\n","다른 분류 알고리즘들도 성능을 test 해보는 것으로!\n","\n","permutation_importances_mean => n_repeats 매개변수로 feature를 10번정도 섞어서 확인을 해본다. => 모델이 어떤 특성을 중심으로 보는 지 알 수 있다.\n","\n","Optuna stratified Kfold"],"metadata":{"id":"4FO2uU8pnK80"}},{"cell_type":"markdown","source":["## TRAIN MODEL"],"metadata":{"id":"EJFnh_9lRY1Z"}},{"cell_type":"code","source":["ROOT_DIR = \"/content/drive/MyDrive/LG AImers/data set/Final Data/\"\n","RANDOM_STATE = 110\n","# Load_train_data_dispenser1\n","dispenser1_df_train = pd.read_csv(os.path.join(ROOT_DIR, \"dispenser1_df_train.csv\"),index_col=0)\n","dispenser2_df_train = pd.read_csv(os.path.join(ROOT_DIR, \"dispenser2_df_train.csv\"),index_col=0)\n","dispenser1_df_test = pd.read_csv(os.path.join(ROOT_DIR, \"dispenser1_df_test.csv\"),index_col=0)\n","dispenser2_df_test = pd.read_csv(os.path.join(ROOT_DIR, \"dispenser2_df_test.csv\"),index_col=0)\n","submission_df = pd.read_csv(os.path.join(\"/content/drive/MyDrive/LG AImers/data set/submission.csv\"))"],"metadata":{"id":"miLF3UmWmOTa","executionInfo":{"status":"ok","timestamp":1724306251148,"user_tz":-540,"elapsed":5791,"user":{"displayName":"이찬","userId":"00141605834555734115"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# dataframe에서 'target'을 기준으로 stratify를 설정하여 데이터 분포를 유지하면서 train/validation split 수행\n","def model_train_test_split(dataframe):\n","  train_x, val_x, train_y, val_y = train_test_split(\n","  dataframe.drop(columns=[\"target\"]),  # feature 데이터\n","  dataframe[\"target\"],                 # target 데이터\n","  test_size=0.3,                      # 검증 데이터 비율\n","  random_state=RANDOM_STATE,          # 랜덤 시드\n","  stratify = dataframe[\"target\"])         # target 값을 기준으로 stratify\n","\n","  return train_x, train_y, val_x, val_y"],"metadata":{"id":"CI5CQoizeQse","executionInfo":{"status":"ok","timestamp":1724306251149,"user_tz":-540,"elapsed":27,"user":{"displayName":"이찬","userId":"00141605834555734115"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["! pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqnfsEu3TZtY","executionInfo":{"status":"ok","timestamp":1724306263401,"user_tz":-540,"elapsed":9277,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"b029626a-2686-4f72-de32-4e4fbcb47c28"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.26.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.1.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.13.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n","Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.5\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import KFold,GridSearchCV,train_test_split,StratifiedKFold\n","from sklearn.metrics import log_loss,f1_score\n","# train validation split\n","from sklearn.model_selection import train_test_split\n","from catboost import CatBoostClassifier, Pool, cv"],"metadata":{"id":"30QM4oEtTlET","executionInfo":{"status":"ok","timestamp":1724306263401,"user_tz":-540,"elapsed":6,"user":{"displayName":"이찬","userId":"00141605834555734115"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from catboost import CatBoostClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer, recall_score\n","\n","def train_catboost_with_gridsearch(dispenser1_df_train, K=5):\n","    df = dispenser1_df_train\n","    df['Chamber Temp. Judge Value_AutoClave'] = df['Chamber Temp. Judge Value_AutoClave'].map({'OK': 1, 'NG': 0})\n","    X_train = df.drop(columns='target')\n","    y_train = df['target']\n","\n","    # Define the CatBoostClassifier\n","    model = CatBoostClassifier(\n","        task_type='GPU',          # Use GPU for training\n","        silent=True\n","    )\n","\n","    # Define the expanded parameter grid for GridSearchCV\n","    param_grid = {\n","        'depth': [4, 6, 8, 10, 12],\n","        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n","        'iterations': [500, 1000, 1500, 2000],\n","        'l2_leaf_reg': [1, 3, 5, 7, 10],\n","        'min_child_samples': [5, 10, 20]\n","    }\n","\n","    # Define the recall scorer\n","    recall_scorer = make_scorer(recall_score)\n","\n","    # Set up GridSearchCV\n","    grid_search = GridSearchCV(\n","        estimator=model,\n","        param_grid=param_grid,\n","        scoring=recall_scorer,\n","        cv=K,\n","        n_jobs=-1\n","    )\n","\n","    # Perform grid search\n","    grid_search.fit(X_train, y_train)\n","\n","    # Return the best model\n","    return grid_search.best_estimator_\n","\n","# Example usage\n","best_model = train_catboost_with_gridsearch(dispenser1_df_train, K=5)\n","\n","print(\"Best model:\", best_model)"],"metadata":{"id":"4kEko1WVokKz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from catboost import CatBoostClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer, recall_score\n","\n","def train_catboost_with_gridsearch(dispenser1_df_train, K=5):\n","    df = dispenser1_df_train\n","    df['Chamber Temp. Judge Value_AutoClave'] = df['Chamber Temp. Judge Value_AutoClave'].map({'OK': 1, 'NG': 0})\n","    X_train = df.drop(columns='target')\n","    y_train = df['target']\n","\n","    # Define the CatBoostClassifier\n","    model = CatBoostClassifier(\n","        task_type='GPU',          # Use GPU for training\n","        silent=True\n","    )\n","\n","    # Define the expanded parameter grid for GridSearchCV\n","    param_grid = {\n","        'depth': [4, 6, 8, 10, 12],\n","        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n","        'iterations': [500, 1000, 1500, 2000],\n","        'l2_leaf_reg': [1, 3, 5, 7, 10],\n","        'min_child_samples': [5, 10, 20]\n","    }\n","\n","    # Define the recall scorer\n","    recall_scorer = make_scorer(recall_score)\n","\n","    # Set up GridSearchCV\n","    grid_search = GridSearchCV(\n","        estimator=model,\n","        param_grid=param_grid,\n","        scoring=recall_scorer,\n","        cv=K,\n","        n_jobs=-1\n","    )\n","\n","    # Perform grid search\n","    grid_search.fit(X_train, y_train)\n","\n","    # Return the best model\n","    return grid_search.best_estimator_\n","\n","# Example usage\n","best_model2 = train_catboost_with_gridsearch(dispenser2_df_train, K=5)\n","\n","print(\"Best model:\", best2_model)"],"metadata":{"id":"fdOXM8OUqbpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1jQgl3k0pdsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qbTHmuapsDFs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4EEEqkNnsDI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"s6vT5ZyhsDMY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> 모델 훈련 시작"],"metadata":{"id":"wHZ58STNdNng"}},{"cell_type":"code","source":["# dispenser 1\n","cat_model_dispenser1 = cat_model_fit(train_x_dispenser1, train_y_dispenser1, val_x_dispenser1, val_y_dispenser1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KN-qhTJkUdSB","executionInfo":{"status":"ok","timestamp":1723944209751,"user_tz":-540,"elapsed":90768,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"fd66c018-c526-4dd2-cd3f-0fe02ce593d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 0.6319386\ttest: 0.6321164\tbest: 0.6321164 (0)\ttotal: 268ms\tremaining: 8m 56s\n","100:\tlearn: 0.2013061\ttest: 0.2129602\tbest: 0.2129563 (98)\ttotal: 13.6s\tremaining: 4m 14s\n","200:\tlearn: 0.1868867\ttest: 0.2116753\tbest: 0.2116753 (200)\ttotal: 26.9s\tremaining: 4m\n","300:\tlearn: 0.1718218\ttest: 0.2116512\tbest: 0.2114121 (247)\ttotal: 44.8s\tremaining: 4m 13s\n","400:\tlearn: 0.1548431\ttest: 0.2131854\tbest: 0.2114121 (247)\ttotal: 1m 12s\tremaining: 4m 47s\n","Stopped by overfitting detector  (250 iterations wait)\n","\n","bestTest = 0.2114120773\n","bestIteration = 247\n","\n","Shrink model to first 248 iterations.\n"]}]},{"cell_type":"code","source":["# dispenser 2\n","cat_model_dispenser2 = cat_model_fit(train_x_dispenser2, train_y_dispenser2, val_x_dispenser2, val_y_dispenser2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IpsACyUUdUo","executionInfo":{"status":"ok","timestamp":1723944254760,"user_tz":-540,"elapsed":45011,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"96148c36-5940-42de-99b7-1f764c6aa2a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 0.6320790\ttest: 0.6320693\tbest: 0.6320693 (0)\ttotal: 63.7ms\tremaining: 2m 7s\n","100:\tlearn: 0.1747951\ttest: 0.1935157\tbest: 0.1934720 (97)\ttotal: 10.2s\tremaining: 3m 10s\n","200:\tlearn: 0.1506364\ttest: 0.1931279\tbest: 0.1927627 (132)\ttotal: 22.2s\tremaining: 3m 18s\n","300:\tlearn: 0.1303781\ttest: 0.1942097\tbest: 0.1927627 (132)\ttotal: 34.7s\tremaining: 3m 15s\n","Stopped by overfitting detector  (250 iterations wait)\n","\n","bestTest = 0.192762664\n","bestIteration = 132\n","\n","Shrink model to first 133 iterations.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5jeRempVCY-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **여기서부터는 prediction threshold, model parameter 조정해보는거야**"],"metadata":{"id":"zzXH8CtKrUkc"}},{"cell_type":"code","source":["# 예측 확률에 AbNormal로 뺴두었던 index 추가하기\n","\n","def add_unconditionally_AbNormal_dispenser1(pred_proba_numpy_array, val_x, submission_abnormal_row_index):\n","  # y_pred_proba1을 DataFrame으로 변환\n","  y_pred_proba_df = pd.DataFrame(y_pred_proba1, index=val_x_dispenser1.index, columns=['probability'])\n","\n","  # Submission_AbNormal_row_index의 인덱스를 y_pred_proba_df에 추가하고, 값을 1로 설정\n","  for idx in submission_abnormal_row_index:\n","    if idx not in y_pred_proba_df.index:\n","      y_pred_proba_df.loc[idx] = 1.0  # 인덱스가 없으면 추가하고 값은 1.0으로 설정\n","\n","  # y_pred_proba_df을 다시 Numpy로 변경\n","  modified_pred_proba_array = y_pred_proba_df.sort_index()['probability'].values\n","\n","  return modified_pred_proba_array"],"metadata":{"id":"YTN7wqlnvFtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# validatoin_y에 AbNormal 추가하기\n","def add_abnormal_rows(val_y, submission_abnormal_row_index):\n","  # val_y의 기존 인덱스\n","  existing_indices = val_y.index.tolist()\n","  # Submission_AbNormal_row_index에서 val_y의 인덱스와 겹치지 않는 인덱스를 필터링\n","  new_indices = [idx for idx in submission_abnormal_row_index if idx not in existing_indices]\n","\n","  # 기존 데이터프레임에 새로운 인덱스를 추가하고 'target' 값을 1로 설정\n","  for idx in new_indices:\n","    val_y.loc[idx] = 1\n","  # 인덱스를 정렬\n","  updated_val_y = val_y.sort_index()\n","\n","  # 인덱스를 정렬\n","  updated_val_y = updated_val_y.values\n","\n","  return updated_val_y"],"metadata":{"id":"jNHVaQLJzTPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예측 - dispenser1\n","y_pred_proba1 = cat_model_dispenser1.predict_proba(val_x_dispenser1)[:, 1]\n","y_pred_proba1 = add_unconditionally_AbNormal_dispenser1(y_pred_proba1, val_x_dispenser1, Submission_AbNormal_row_index)\n","y_pred1 = (y_pred_proba1 > 0.3).astype(int)\n","updated_val_y = add_abnormal_rows(val_y_dispenser1, Submission_AbNormal_row_index)\n","# F1 스코어 계산\n","f1 = f1_score(updated_val_y, y_pred1)\n","print(f'F1 Score: {f1:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIUl2p0mrTjx","executionInfo":{"status":"ok","timestamp":1723902996208,"user_tz":-540,"elapsed":366,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"175a0d9d-75aa-47f6-b204-96d658e8b02d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.3449\n"]}]},{"cell_type":"code","source":["# 예측 확률에 AbNormal로 뺴두었던 index 추가하기\n","\n","def add_unconditionally_AbNormal_dispenser2(pred_proba_numpy_array, val_x, submission_abnormal_row_index):\n","  # y_pred_proba1을 DataFrame으로 변환\n","  y_pred_proba_df = pd.DataFrame(y_pred_proba2, index=val_x_dispenser2.index, columns=['probability'])\n","\n","  # Submission_AbNormal_row_index의 인덱스를 y_pred_proba_df에 추가하고, 값을 1로 설정\n","  for idx in submission_abnormal_row_index:\n","    if idx not in y_pred_proba_df.index:\n","      y_pred_proba_df.loc[idx] = 1.0  # 인덱스가 없으면 추가하고 값은 1.0으로 설정\n","\n","  # y_pred_proba_df을 다시 Numpy로 변경\n","  modified_pred_proba_array = y_pred_proba_df.sort_index()['probability'].values\n","\n","  return modified_pred_proba_array"],"metadata":{"id":"FizdkjZm-2Wu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예측 - dispenser2\n","y_pred_proba2 = cat_model_dispenser2.predict_proba(val_x_dispenser2)[:, 1]\n","y_pred_proba2 = add_unconditionally_AbNormal_dispenser2(y_pred_proba2, val_x_dispenser2, Submission_AbNormal_row_index)\n","y_pred2 = (y_pred_proba2 > 0.3).astype(int)\n","updated_val_y_2 = add_abnormal_rows(val_y_dispenser2, Submission_AbNormal_row_index)\n","# F1 스코어 계산\n","f2 = f1_score(updated_val_y_2, y_pred2)\n","print(f'F1 Score: {f2:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZqXPdxGprTmN","executionInfo":{"status":"ok","timestamp":1723902811373,"user_tz":-540,"elapsed":321,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"e768911b-25ff-4d97-f1eb-d232180c6d47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.4989\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Lj16D0CWrTvp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analizing Prediction Result_TRAIN"],"metadata":{"id":"4S5AFzaHUHYw"}},{"cell_type":"code","source":["train_x_dispenser1 = convert_to_category(train_x_dispenser1)\n","\n","# 특성 중요도 계산 및 출력\n","category_columns = train_x_dispenser1.select_dtypes(include='category').columns.tolist()\n","feature_importances = cat_model_dispenser1.get_feature_importance(Pool(train_x_dispenser1, label=train_y_dispenser1, cat_features=category_columns))\n","\n","# 특성 중요도를 내림차순으로 정렬하고 상위 30개의 인덱스를 선택\n","sorted_idx = np.argsort(feature_importances)[::-1]  # 내림차순으로 정렬된 인덱스\n","top_40_idx = sorted_idx[:40]  # 상위 30개의 인덱스 선택\n","\n","# 상위 30개의 특성과 중요도만 선택\n","top_40_features = train_x_dispenser1.columns[top_40_idx]\n","top_40_importances = feature_importances[top_40_idx]\n","\n","# 상위 30개 특성에 대해 시각화\n","plt.figure(figsize=(10, 10))\n","plt.barh(top_40_features, top_40_importances)\n","plt.xlabel('Feature Importance')\n","plt.ylabel('Features')\n","plt.title('Top 40 Feature Importance in CatBoost Model')\n","plt.show()"],"metadata":{"id":"-gQ9VGLcRphI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PREDICT & SCORE => Test data"],"metadata":{"id":"SJC1-O9tUCI6"}},{"cell_type":"code","source":["dispenser1_df_test.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5QL_Y2JJkM9","executionInfo":{"status":"ok","timestamp":1724046029186,"user_tz":-540,"elapsed":6,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"15bf0eed-b176-421b-b9c0-748561370076"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10731"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["dispenser2_df_test.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mc3zq6gtJq5z","executionInfo":{"status":"ok","timestamp":1724046037658,"user_tz":-540,"elapsed":574,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"de96959e-9c5f-40f5-a0b4-182dd651887c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6618"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["y_pred_proba1_test = cat_model_dispenser1.predict_proba(dispenser1_df_test)[:, 1]\n","y_pred1_test = (y_pred_proba1_test > 0.11).astype(int)\n","\n","y_pred_proba2_test = cat_model_dispenser2.predict_proba(dispenser2_df_test)[:, 1]\n","y_pred2_test = (y_pred_proba2_test > 0.11).astype(int)\n","\n","# submission에 대입\n","submission_df.loc[dispenser1_df_test.index, 'target'] = y_pred1_test\n","submission_df.loc[dispenser2_df_test.index, 'target'] = y_pred2_test\n","submission_df['target'] = submission_df['target'].map({0: 'Normal', 1: 'AbNormal'})\n","submission_df.loc[Submission_AbNormal_row_index_test, 'target'] = \"AbNormal\""],"metadata":{"id":"FTGnIgJ7RpbM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# predict 임계값 : 0.11, depth : 5 , earlystopping : 40  ==> 0.185\n","submission_df[\"target\"].value_counts()\n","# 15:1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"yzhkb2swUNHe","executionInfo":{"status":"ok","timestamp":1723944320736,"user_tz":-540,"elapsed":563,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"be0a8443-d07c-47e2-859d-74b00a8acf35"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["target\n","Normal      16276\n","AbNormal     1085\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>target</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Normal</th>\n","      <td>16276</td>\n","    </tr>\n","    <tr>\n","      <th>AbNormal</th>\n","      <td>1085</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["submission_df.to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"IIc-6N5IEC2O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data[\"target\"].value_counts()\n","# 16 : 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"isBwNmfyUYza","executionInfo":{"status":"ok","timestamp":1723893483073,"user_tz":-540,"elapsed":357,"user":{"displayName":"이찬","userId":"00141605834555734115"}},"outputId":"90602dcb-5d7e-40e0-940d-f3d610188824"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["target\n","Normal      38156\n","AbNormal     2350\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>target</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Normal</th>\n","      <td>38156</td>\n","    </tr>\n","    <tr>\n","      <th>AbNormal</th>\n","      <td>2350</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":117}]},{"cell_type":"code","source":[],"metadata":{"id":"Metyzz6EUY2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"olqVt-xgUY43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vvu5rB7AFnZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VZqD1zKGCUtQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XGdrdYRWCUwW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bGiVS-AYCUyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m4PYHGTdCU06"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i4IK1wSACU3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vRubxNwfCU55"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XUOuZnsHCU80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GeqM3SL7CU_C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> #### 데이터를 고쳤는데도 점수가 거의 오르지 않은 것으로 보아, 확실히 AbNormal인 데이터들이 적은데 빼니까 더 감지가 어려운듯함. 처음부터 제외하고 학습을 시키기 보다는 같이 학습을 시키는 것이 좋아보임"],"metadata":{"id":"g2zoYUqG6Iah"}},{"cell_type":"markdown","source":["### 1. undersampling"],"metadata":{"id":"NPoLPs1pm4YG"}},{"cell_type":"code","source":[],"metadata":{"id":"KM_r9l09FncU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x62ulKloFne6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2xTcG0cDUY7q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 여기부터는 생각 더!"],"metadata":{"id":"A_5BKFJOTOap"}},{"cell_type":"markdown","source":["### STEP 특징을 살펴보고 unique한 값이 오직 하나면 drop할 지 결정하자.\n","\n","- CURE START POSITION Θ Collect Result_Dam == CURE END POSITION Θ Collect Result_Dam\n","- 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam' => stage 1,2,3가 동일\n","- HeadNormal Y_dam도 stage 1,3가 3개 제외하고는 다 같은 듯\n","\n","> **조금 특이**\n","- 'HEAD Standby Position X Collect Result_Dam' ==  ['Head Purge Position X Collect Result_Dam'],\n","- 'HEAD Standby Position Y Collect Result_Dam' ==  ['Head Purge Position Y Collect Result_Dam'],\n","- **'HEAD Standby Position Z Collect Result_Dam' !=  ['Head Purge Position Z Collect Result_Dam'],**\n","- 'HEAD Standby Position Z Collect Result_Dam' == ['HEAD Standby Position Z Collect Result_Fill1'], => **얘는 다른 이유가 뭘까**\n","\n","### Column의 unique한 값이 1개뿐인 column"],"metadata":{"id":"vM0RLf-bLJWA"}},{"cell_type":"markdown","source":["### STEP 5. 모든 공정에서 값이 동일한 column drop ==> 딱 1개만 남겨두는 방향으로!\n","- Model suffix\n","- workorder"],"metadata":{"id":"qaO7sMIkM_GA"}},{"cell_type":"markdown","source":["> 데이터를 다시 해보니, 값이 아예 동일한 컬럼들이 존재하네?"],"metadata":{"id":"cA0HQkdy8pYh"}},{"cell_type":"markdown","source":["## g"],"metadata":{"id":"A9dP2r2xQGlR"}},{"cell_type":"markdown","source":["- work order의 1234는 의미가 있는 숫자일까?\n","- chamber temp 가 중간인 곳과 아닌 곳으로 chamber temp가 NG OK 구분됨\n","- Machine tact time이 정상 비정상에서 차이가 크다\n","- Qty가 0은 무슨 의미이지?\n","- 1st Pressure 1st Pressure Unit Time_AutoClave 가 0인 값들이 있네? 1st & 2nd\n","- 이때 1st, 2nd가 둘 다 time이 0인 값은 없어\n","- 3rd는 무조건 1초이상인데!\n","\n","\n",">- 압력의 범위가 Normal일 때가 더 넓어\n","- **Abnormal 일때는 더 넓어야 하는데, 그러지 못해서 그랬을 확률이 있어보여.**\n","\n","\n","> Dam dispenser에 따라 CURE START & END POSITION & HEAD NORMAL COORDINATE 가 달라진\n","- head는 분명한 순서가 존재하는  과정\n","\n","- model.SuffixDam은 디스플레이 모델을 구분하는 것이라면... 이게 dam 높이라던지 레진 도포 위치 등의 공정의 좌표를 결정하지 않을까?"],"metadata":{"id":"CxXlNGpmLlg8"}},{"cell_type":"code","source":[],"metadata":{"id":"5aJyONGKKDEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XOR5sPoaKDHY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qV4XU4KBKDKe"},"execution_count":null,"outputs":[]}]}